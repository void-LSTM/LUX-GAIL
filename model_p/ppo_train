
import torch
import argparse
import argparse

from env.lux_env import ProxyEnv, Observation
from opponent.lux_ai.rl_agent.rl_agent import agent as opAgent
from agent_ import Agent, STATE_CHANNELS
from model.policy_network import PolicyNetwork
from config import *
from agent_constants import *
from model.policy_network import PolicyNetwork,Critic
from model.teacher_network import PolicyNetwork_teacher
from model.feature import feature_net
from test import EvalEnv
import argparse


torch.autograd.set_detect_anomaly(True)
from torch.utils.tensorboard import SummaryWriter  

from torchsummary import summary
device = torch.device('cuda')
teacher=PolicyNetwork_teacher(in_channels=STATE_CHANNELS, feature_size=FEATURE_SIZE, layers=LAYERS,
                        num_unit_actions=UNIT_ACTIONS, num_citytile_actions=CITYTILE_ACTIONS).cuda()
checkpoint = torch.load('D:\LuxAI-main\checkpoint.pth')
teacher.load_state_dict(checkpoint['model'])
state_model = feature_net(in_channels=STATE_CHANNELS, feature_size=FEATURE_SIZE, layers=LAYERS)
save_model = teacher
model_dict =  state_model.state_dict()   
#print(model_dict)
state_dict = {k:v for k,v in save_model.state_dict().items() if k in model_dict.keys()}
model_dict.update(state_dict)
state_model.load_state_dict(model_dict)
state_model.cuda()
policy_net = PolicyNetwork(feature_size=384, num_unit_actions=10,
                        num_citytile_actions=3)
checkpoint_policy = torch.load('D:\LuxAI-main-ppo\ppo_checkpoint.pth')
policy_net.load_state_dict(checkpoint_policy['model'])
policy_net.cuda()
critic=Critic(feature_size=FEATURE_SIZE).cuda()
optimizer_td = torch.optim.Adam(critic.parameters(),
                                        lr=1e-2,weight_decay=WEIGHT_DECAY)
def get_prob(actions,action_probs,targets):
    b,w,h=actions.shape
    actions=actions.reshape(b,w,h,1)
    action_probs = action_probs.gather(3, actions)
    action_probs=action_probs.reshape(b,-1)
    targets=targets.reshape(b,-1)
    action_probs=action_probs*targets
    action_probs=torch.sum(action_probs,1)
    action_probs=action_probs.reshape(b,-1)
    return action_probs
def get_data():
    
    states = []
    marks=[]
    rewards = []
    actions = []
    next_states = []
    next_masks=[]
    overs = []
    env = ProxyEnv(map_size=32)
    conf = env.conf
    obs, _, done = env.reset()
    step = 0
    agent0=Agent(policy_net, state_model,device, research_th=0.05, research_turn=15)
    agent1=opAgent
    while not done:
        observation = Observation()
        observation["updates"] = obs[0]["updates"]
        observation["remainingOverageTime"] = 60.
        observation["step"] = step
        actions = [[],[]]
        observation.player = 0
        actions[0],state,mask = agent0(observation,conf)
        observation.player = 1
        actions[1] = agent1(observation,conf)
        obs, _,reward, done = env.step(actions)
        overs.append(done)
        step += 1
        states.append(state)
        marks.append(mask)
        rewards.append(reward)
        actions.append(actions)
        next_state=observation
        next_state["updates"] = obs[0]["updates"]
        next_state["remainingOverageTime"] = 60.
        next_state["step"] = step
        _,next_state,next_mask = agent0(next_state,conf)
        next_states.append(next_state)   
        next_masks.append(next_mask)     
    return states, marks,rewards, actions, next_states, next_masks,overs

def train():
    parser = argparse.ArgumentParser(description='Lux AI Evaluation')
    parser.add_argument('--num_games', '-n', help='the number of games', default=1, type=int)
    parser.add_argument('--map_size', '-s', help='the size of map', default=12,type=int)
    parser.add_argument('--seed', '-r', help='the random seed', default=42, type=int)
    args = parser.parse_args()    
    optimizer = torch.optim.AdamW(policy_net.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY)
    optimizer_state = torch.optim.AdamW(state_model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY)
    optimizer_td = torch.optim.Adam(critic.parameters(),
                                            lr=1e-2,weight_decay=WEIGHT_DECAY)
    loss_fn = torch.nn.MSELoss()
    #玩N局游戏,每局游戏训练M次
    for epoch in range(3000):
        states, marks,rewards, actions, next_states, next_masks,overs = get_data()
        print(states)
        feature=state_model(states,marks)
        next_feature=state_model(next_states,next_masks)
        rewards = (rewards + 8) / 8
        critic_value=critic(feature)
        targets = critic(next_feature).detach()
        targets = targets * 0.98
        targets *= (1 - overs)
        targets += rewards
        #[b, 1]
        deltas = (targets - critic_value).squeeze(dim=1).tolist()
        advantages = []
        #反向遍历deltas
        s = 0.0
        for delta in deltas[::-1]:
            s = 0.98 * 0.95 * s + delta
            advantages.append(s)

        #逆序
        advantages.reverse()
        advantages = torch.FloatTensor(advantages).reshape(-1, 1).to(device)
        advantages.requires_grad=True

        (actions2, action_probs, _), (citytile_actions2, citytile_action_probs, _) = policy_net.act(feature,states[:,25,:,:])
        unit_action_old_porb=get_prob(actions2,action_probs,states[:,0,:,:])
        citytile_action_old_porb=get_prob(citytile_actions2,citytile_action_probs,states[:,19,:,:])
        old_porb=unit_action_old_porb+citytile_action_old_porb
        old_porb=old_porb.detach()
        #每批数据反复训练10次
        for _ in range(1):
            
            (actions2, action_probs, _), (citytile_actions2, citytile_action_probs, _) = policy_net.act(feature,states[:,25,:,:])
            unit_action_new_porb=get_prob(actions2,action_probs,states[:,0,:,:])
            citytile_action_new_porb=get_prob(citytile_actions2,citytile_action_probs,states[:,19,:,:])
            
            
            new_prob=unit_action_new_porb+citytile_action_new_porb+1e-16
            ratios = old_porb / new_prob
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 0.8, 1.2) * advantages
            loss_p = (-torch.min(surr1, surr2)).mean()
            

            critic_value=critic(feature)
            loss_critic = loss_fn(critic_value, targets)

            
            #更新参数
            optimizer_td.zero_grad()
            loss_critic.backward()
            optimizer_td.step()

            optimizer.zero_grad()
            loss_p.backward()
            optimizer.step()

            optimizer_state.zero_grad()
            loss_p.backward()
            optimizer_state.step()
        if epoch % 200 == 0:
            eval_env = EvalEnv(args)
            eval_env.run()       
if __name__ == '__main__':
    train()




